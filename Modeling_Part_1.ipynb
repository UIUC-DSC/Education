{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19b9a5ed",
      "metadata": {},
      "source": [
        "# Modeling (Part 1)\n",
        "\n",
        "We will talk about:\n",
        "\n",
        "1. Supervised learning, unsupervised learning\n",
        "2. Regression, classification, clustering\n",
        "3. Linear regression, Logistic regression, K-Means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "361cd6b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing, load_breast_cancer, load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error, r2_score,\n",
        "    accuracy_score, classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "np.random.seed(42) # this is used to make the results reproducible"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8cc0daf",
      "metadata": {},
      "source": [
        "# Supervised Learning\n",
        "\n",
        "In supervised learning:\n",
        "\n",
        "- We have data ($X$) and the correct answers (label $y$).\n",
        "- The goal is to teach the model to use $X$ to predict $y$ for new examples.\n",
        "\n",
        "There are two main types, and the difference is:\n",
        "\n",
        "- **Regression**: Predict a number (e.g., house price)\n",
        "- **Classification**: Predict a category (e.g., yes/no, red/blue)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86dcc0fc",
      "metadata": {},
      "source": [
        "## Linear Regression\n",
        "\n",
        "### Mathematical Intuition\n",
        "\n",
        "For single-variable linear regression:\n",
        "\n",
        "$$\\hat{y} = wx + b$$\n",
        "\n",
        "We measure error with **Mean Squared Error (MSE)**:\n",
        "\n",
        "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2$$\n",
        "\n",
        "Goal: find $w$ and $b$ that minimize MSE."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14979352",
      "metadata": {},
      "source": [
        "We will use the *California Housing* dataset from scikit-learn to perform linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7276d24f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Our demo uses a real-world dataset from scikit-learn: California Housing\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "X_housing = housing.data\n",
        "y_housing = housing.target\n",
        "\n",
        "print('Feature shape:', X_housing.shape)\n",
        "print('Target shape:', y_housing.shape)\n",
        "display(X_housing.head())\n",
        "display(y_housing.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b693346f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single-feature version for visualization: use MedInc to predict house value\n",
        "X_single = X_housing[['MedInc']]\n",
        "y = y_housing\n",
        "\n",
        "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(\n",
        "    ...\n",
        ")\n",
        "\n",
        "linreg_single = ...\n",
        "...\n",
        "y_pred_s = ...\n",
        "\n",
        "mse_s = mean_squared_error(y_test_s, y_pred_s)\n",
        "r2_s = r2_score(y_test_s, y_pred_s)\n",
        "\n",
        "print(f'Single-feature model: y_hat = {linreg_single.coef_[0]:.3f} * MedInc + {linreg_single.intercept_:.3f}')\n",
        "print(f'MSE: {mse_s:.3f}')\n",
        "print(f'R^2: {r2_s:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c9712c2",
      "metadata": {},
      "source": [
        "### What is $R^2$?\n",
        "\n",
        "$R^2$ (coefficient of determination) represents how well the model explains the variance in the target variable.\n",
        "\n",
        "$R^2 = 0.459$ means that about 45.9% of the variation in house prices can be explained by the single feature MedInc (median income), while the remaining 54.1% of the variance is due to other factors or noise that the model does not capture."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c74dd3e2",
      "metadata": {},
      "source": [
        "Below we visualize the fitted regression line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14a4da26",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regression line visualization\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X_test_s['MedInc'].values, y_test_s.values, alpha=0.3, s=10, label='Data')\n",
        "plt.plot(X_test_s['MedInc'].values, y_pred_s, color='red', linewidth=2, linestyle='dashed', label='Fitted regression line')\n",
        "plt.xlabel('Median Income (MedInc)')\n",
        "plt.ylabel('Median House Value')\n",
        "plt.title('Linear Regression on California Housing')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfcbc6cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-feature linear regression using all available features\n",
        "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
        "    X_housing, y_housing, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "linreg_multi = LinearRegression()\n",
        "linreg_multi.fit(X_train_m, y_train_m)\n",
        "y_pred_m = linreg_multi.predict(X_test_m)\n",
        "\n",
        "mse_m = ...\n",
        "r2_m = ...\n",
        "\n",
        "print(f'Multi-feature MSE:  {mse_m:.3f}')\n",
        "print(f'Multi-feature R^2:  {r2_m:.3f}')\n",
        "\n",
        "coef_table = pd.DataFrame({\n",
        "    'feature': X_housing.columns,\n",
        "    'coefficient': linreg_multi.coef_\n",
        "}).sort_values('coefficient', ascending=False)\n",
        "\n",
        "display(coef_table)\n",
        "\n",
        "print(f'\\nIntercept: {linreg_multi.intercept_:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54295b98",
      "metadata": {},
      "source": [
        "**Equation:**\n",
        "\n",
        "$$\n",
        "\\hat{y} = -37.023278\n",
        "+ 0.7831 \\cdot \\text{AveBedrms}\n",
        "+ 0.4487 \\cdot \\text{MedInc}\n",
        "+ 0.0097 \\cdot \\text{HouseAge}\n",
        "$$\n",
        "$$\n",
        "- 0.000002 \\cdot \\text{Population}\n",
        "- 0.0035 \\cdot \\text{AveOccup}\n",
        "- 0.1233 \\cdot \\text{AveRooms}\n",
        "$$\n",
        "$$\n",
        "- 0.4198 \\cdot \\text{Latitude}\n",
        "- 0.4337 \\cdot \\text{Longitude}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a925a9e5",
      "metadata": {},
      "source": [
        "## Logistic Regression\n",
        "\n",
        "### Mathematical Intuition\n",
        "\n",
        "Logistic Regression fits a linear equation to the data, then squeezes the output through a **Sigmoid function** to produce a probability between 0 and 1, which is used to classify data into categories."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de9e1bcf",
      "metadata": {},
      "source": [
        "Let's visualize the sigmoid function first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4883ae3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "z = np.linspace(-10, 10, 200)\n",
        "sigmoid = 1 / (1 + np.exp(-z))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(z, sigmoid, color='blue', linewidth=2)\n",
        "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Threshold = 0.5')\n",
        "plt.axvline(x=0, color='gray', linestyle=':', alpha=0.5)\n",
        "plt.xlabel('z (linear combination)', fontsize=12)\n",
        "plt.ylabel('σ(z) = 1 / (1 + e⁻ᶻ)', fontsize=12)\n",
        "plt.title('Sigmoid Function', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88e4a367",
      "metadata": {},
      "source": [
        "Next, we will perform logistic regression on the *Breast Cancer Wisconsin* dataset from scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a218a5c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Real-world dataset from scikit-learn: Breast Cancer Wisconsin\n",
        "cancer = load_breast_cancer(as_frame=True)\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "print('Classes:', cancer.target_names)\n",
        "print()\n",
        "print('Shape:', X_cancer.shape)\n",
        "print()\n",
        "print(y_cancer.value_counts())\n",
        "print()\n",
        "display(X_cancer.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6939733",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
        ")\n",
        "\n",
        "# Standardize features because logistic regression is scale-sensitive\n",
        "scaler = ...\n",
        "X_train_c_scaled = ...\n",
        "X_test_c_scaled = ...\n",
        "\n",
        "logreg = LogisticRegression(random_state=42)\n",
        "logreg.fit(X_train_c_scaled, y_train_c)\n",
        "y_pred_c = logreg.predict(X_test_c_scaled)\n",
        "\n",
        "acc = accuracy_score(y_test_c, y_pred_c)\n",
        "print(f'Accuracy: {acc:.4f}')\n",
        "print()\n",
        "print('Classification report:')\n",
        "print(classification_report(y_test_c, y_pred_c, target_names=cancer.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff2d1348",
      "metadata": {},
      "source": [
        "### What are these evaluation metrics?\n",
        "- Accuracy: The percentage of all predictions that the model got right.\n",
        "- Precision: Out of everything the model predicted as positive, how many were actually positive. (How trustworthy are the \"yes\" predictions?)\n",
        "- Recall: Out of all the actual positives, how many did the model successfully catch. (How good is the model at finding all the real positives?)\n",
        "- F1-score: A single number that balances Precision and Recall — useful when you care about both equally.\n",
        "- Support: Simply the number of actual samples in each class — it's not a metric, just a count telling you how many real examples of each class exist in the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "022c1af6",
      "metadata": {},
      "source": [
        "**Confusion matrix** is also a good visualization plot to see the performence of a binary classification task, including all the metrics above, implicitly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3f8f41b",
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = ...\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "ConfusionMatrixDisplay.from_predictions(y_test_c, y_pred_c, display_labels=cancer.target_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e10b7de6",
      "metadata": {},
      "source": [
        "# Unsupervised Learning\n",
        "\n",
        "Unsupervised learning means:\n",
        "\n",
        "- We only have input features $X$ and no labels $y$.\n",
        "- We want to discover hidden structure in data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "481fed20",
      "metadata": {},
      "source": [
        "## K-Means Clustering\n",
        "\n",
        "Algorithm intuition:\n",
        "\n",
        "1. Initialize $K$ centroids\n",
        "2. Assign each point to its nearest centroid\n",
        "3. Update centroids as cluster means\n",
        "4. Repeat steps 2-3 until convergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d667166",
      "metadata": {},
      "outputs": [],
      "source": [
        "##### YOU DON\"T NEED TO UNDERSTAND THIS CODE, IT IS JUST FOR ILLUSTRATION #####\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "np.random.seed(11111)\n",
        "X = np.vstack([\n",
        "    np.random.randn(50, 2) + [2, 2],\n",
        "    np.random.randn(50, 2) + [-2, -2],\n",
        "    np.random.randn(50, 2) + [2, -2]\n",
        "])\n",
        "\n",
        "k = 3\n",
        "centroids = X[np.random.choice(len(X), k, replace=False)]\n",
        "\n",
        "history_centroids = [centroids.copy()]\n",
        "history_labels = []\n",
        "\n",
        "for _ in range(10):\n",
        "    distances = np.linalg.norm(X[:, None] - centroids[None, :], axis=2)\n",
        "    labels = np.argmin(distances, axis=1)\n",
        "    history_labels.append(labels.copy())\n",
        "\n",
        "    new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n",
        "    if np.allclose(centroids, new_centroids):\n",
        "        break\n",
        "    centroids = new_centroids\n",
        "    history_centroids.append(centroids.copy())\n",
        "\n",
        "colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "def animate(frame):\n",
        "    ax.clear()\n",
        "    step = frame // 2\n",
        "    phase = frame % 2\n",
        "\n",
        "    if step < len(history_labels):\n",
        "        labels = history_labels[step]\n",
        "        for i in range(k):\n",
        "            ax.scatter(X[labels == i, 0], X[labels == i, 1],\n",
        "                       c=colors[i], alpha=0.5, s=30)\n",
        "\n",
        "        if phase == 0:\n",
        "            c = history_centroids[step]\n",
        "            ax.set_title(f'Step {step + 1}: Assign points to nearest centroid', fontsize=13)\n",
        "        else:\n",
        "            c = history_centroids[min(step + 1, len(history_centroids) - 1)]\n",
        "            ax.set_title(f'Step {step + 1}: Update centroids', fontsize=13)\n",
        "\n",
        "        ax.scatter(c[:, 0], c[:, 1], c='black', marker='X', s=200,\n",
        "                   edgecolors='white', linewidths=2, zorder=5)\n",
        "\n",
        "    ax.set_xlim(X[:, 0].min() - 1, X[:, 0].max() + 1)\n",
        "    ax.set_ylim(X[:, 1].min() - 1, X[:, 1].max() + 1)\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "\n",
        "total_frames = len(history_labels) * 2\n",
        "anim = FuncAnimation(fig, animate, frames=total_frames, interval=800, repeat=True)\n",
        "plt.close(fig)\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6bdb7cf",
      "metadata": {},
      "source": [
        "We will use the *Iris* dataset from scikit-learn, to perform K-Means clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81f48cc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Real-world dataset from scikit-learn: Iris\n",
        "iris = load_iris(as_frame=True)\n",
        "X_iris_full = iris.data\n",
        "y_iris_true = iris.target\n",
        "\n",
        "# We use two features for simple 2D visualization\n",
        "X_iris = X_iris_full[['petal length (cm)', 'petal width (cm)']]\n",
        "X_iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2501ad92",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "plt.scatter(X_iris.iloc[:, 0], X_iris.iloc[:, 1], alpha=0.7)\n",
        "plt.xlabel('Petal length (cm)')\n",
        "plt.ylabel('Petal width (cm)')\n",
        "plt.title('Iris Data (No labels used in training)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec43c3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose K=3 (a common choice for Iris)\n",
        "kmeans = ... # here you can also try different values of K\n",
        "cluster_labels = ...\n",
        "centers = ...\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.scatter(X_iris.iloc[:, 0], X_iris.iloc[:, 1], c=cluster_labels, cmap='viridis', alpha=0.75)\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=180, marker='X', label='Centroids')\n",
        "plt.xlabel('Petal length (cm)')\n",
        "plt.ylabel('Petal width (cm)')\n",
        "plt.title('K-Means Clustering Result (K=3)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a18bb2b2",
      "metadata": {},
      "source": [
        "### How should we evaluate the performance?\n",
        "\n",
        "We can calculate the within-cluster sum of sqaures (WCSS) for each cluster which combines the distance (of each point) from the points in its cluster and the distance to points outside of its cluster. \n",
        "\n",
        "$$\\sum_{i=0}^n \\min_{\\mu_j \\epsilon C}(||x_i - \\mu_j||^2)$$\n",
        "\n",
        "Ideally, we want to minimize this value so that all our clusters are about the same size. However, this number will always decrease as we increase the number of clusters. (Think about what would happen if k = number of data points.) So, we might choose the ideal number of clusters as that which marks the final steep decrease in the WCSS. It's easier to see this with a plot:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0505c519",
      "metadata": {},
      "outputs": [],
      "source": [
        "wcss = []\n",
        "for i in range(1, 8):\n",
        "    kmeans = KMeans(n_clusters=i)\n",
        "    kmeans.fit(X_iris)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(range(1, 8), wcss)\n",
        "plt.grid()\n",
        "plt.title('Elbow Plot')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Within Cluster Sum of Squares')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22c48e72",
      "metadata": {},
      "source": [
        "After k = 3, the reduction in the WCSS is quite small. There are other ways to check for the ideal k value, such as visualizing the density of each cluster with the [silhouette coefficient](https://www.scikit-yb.org/en/latest/api/cluster/silhouette.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4fa376",
      "metadata": {},
      "source": [
        "Key takeaways:\n",
        "\n",
        "1. Supervised learning uses labeled data and includes regression/classification tasks\n",
        "2. Linear regression predicts continuous values; logistic regression predicts which class a data point belongs to\n",
        "3. Unsupervised learning discovers structure without labels, and K-Means is a classic clustering method"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
