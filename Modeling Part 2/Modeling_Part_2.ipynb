{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d14d0f",
   "metadata": {},
   "source": [
    "# Training, Tuning, and Evaluating Models in Python\n",
    "\n",
    "In this notebook, we will demonstrate:\n",
    "1. How to train and evaluate models using scikit-learn `model.fit` and `model.predict`.\n",
    "2. Great way to tune hyperparameters using `GridSearchCV`.\n",
    "\n",
    "We will use a dataset with both categorical and numerical features to showcase preprocessing steps, model training, and evaluation. \n",
    "\n",
    "For the purposes of instruction, we're making some poor decisions when it comes to preprocessing and modeling. (We really don't need to be using PCA.) Be sure to understand the *why* of each step during your data dive project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16b236",
   "metadata": {},
   "source": [
    "If you don't have the following packages installed, delete the # and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc7449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install sci-kit learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac74493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca08107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "df = pd.read_csv(\"insurance_with_missing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2736fc4b",
   "metadata": {},
   "source": [
    "## Dataset Overview – Medical Insurance Charges\n",
    "\n",
    "### Response\n",
    "\n",
    "- `charges`: **[float64]** – Individual medical costs billed by health insurance (in USD)\n",
    "\n",
    "### Features\n",
    "\n",
    "- `age`: **[int64]** – Age of the primary insurance beneficiary (in years)\n",
    "\n",
    "- `sex`: **[object]** – Gender of the insurance contractor (`male`, `female`)\n",
    "\n",
    "- `bmi`: **[float64]** – Body Mass Index (kg/m²), calculated as weight divided by height squared  \n",
    "  *(Healthy range: 18.5 – 24.9)*\n",
    "\n",
    "- `children`: **[int64]** – Number of dependents covered by health insurance\n",
    "\n",
    "- `smoker`: **[object]** – Smoking status (`yes`, `no`)\n",
    "\n",
    "- `region`: **[object]** – Residential region in the U.S. (`northeast`, `southeast`, `southwest`, `northwest`)\n",
    "\n",
    "### Problem Type\n",
    "\n",
    "- Supervised Learning\n",
    "- Regression (predicting a continuous target variable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b17f412",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadde6e5",
   "metadata": {},
   "source": [
    "All of our data types look as we expect them to, so we can proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc221c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98115a",
   "metadata": {},
   "source": [
    "We have multiple columns with NA values. We will need to fix this before the modeling stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444cd9ca",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f85405",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['age', 'sex', 'bmi', 'children', 'smoker', 'region']]\n",
    "y = df['charges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ef8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_region = ...\n",
    "\n",
    "X.loc[:,'region'] = imputer_region.fit_transform(X[['region']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1a0d8",
   "metadata": {},
   "source": [
    "For the categorical variables, we can replace the NA values with the most frequent value in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233a61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_mean = ...\n",
    "\n",
    "X.loc[:,'age'] = imputer_mean.fit_transform(X[['age']])\n",
    "X.loc[:,'bmi'] = imputer_mean.fit_transform(X[['bmi']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14e3e1",
   "metadata": {},
   "source": [
    "For the numeric variables, we can replace the NA values with the mean of the data points. (This is not always a good idea! Be sure to understand the consequences of imputation before applying it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fca783",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0429cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ...\n",
    "\n",
    "encoded_categorical = encoder.fit_transform(X[['sex', 'smoker', 'region']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_categorical_df = pd.DataFrame(encoded_categorical.toarray(), columns=encoder.get_feature_names_out())\n",
    "encoded_categorical_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34bff7c",
   "metadata": {},
   "source": [
    "Since the sk-learn models cannot handle categorical data, we need to represent it as binary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8543907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=['sex', 'smoker', 'region'], axis=1)\n",
    "X = ...\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a1218",
   "metadata": {},
   "source": [
    "Then we can remove our categorical columns and add the binary columns to our original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee03079e",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7346bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faaee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31787c8e",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8665a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = ...\n",
    "\n",
    "X_train[['age', 'bmi', 'children']] = scaler.fit_transform(X_train[['age', 'bmi', 'children']])\n",
    "X_test[['age', 'bmi', 'children']] = scaler.transform(X_test[['age', 'bmi', 'children']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd76964",
   "metadata": {},
   "source": [
    "We want to scale our variables after the data has been split into training and test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61271ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0494ce2",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef61ca9",
   "metadata": {},
   "source": [
    "Should we be using PCA with one-hot encoded variables? Maybe not. But let's do it anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530bf830",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16440ad2",
   "metadata": {},
   "source": [
    "We want to keep 95% of the variance in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b084f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df326807",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of components selected:\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3a2e8",
   "metadata": {},
   "source": [
    "We only need 7 principal components (columns) instead of our original 10 columns to keep 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Explained variance ratio:\")\n",
    "print(...) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8f498",
   "metadata": {},
   "source": [
    "This tells us how much variance each of the principal components explains. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde40a0b",
   "metadata": {},
   "source": [
    "Wow, that was a lot of work! Is there an easier way to perform all of these steps? Hmm..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066466c4",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81379bae",
   "metadata": {},
   "source": [
    "### L2 Penalty Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe1006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "ridge = ...\n",
    "\n",
    "# Fit and predict \n",
    "ridge.fit(X_train_pca, y_train)\n",
    "ridge_preds = ridge.predict(X_test_pca)\n",
    "\n",
    "# Evaluate \n",
    "ridge_rmse = root_mean_squared_error(y_test, ridge_preds)\n",
    "ridge_r2 = r2_score(y_test, ridge_preds)\n",
    "\n",
    "print(\"Ridge RMSE:\", ridge_rmse)\n",
    "print(\"Ridge R2:\", ridge_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145270e",
   "metadata": {},
   "source": [
    "The `alpha` parameter sets the strength of the penalty term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561aede",
   "metadata": {},
   "source": [
    "### L1 Penalty Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fdcda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "lasso = ...\n",
    "\n",
    "# Fit and predict \n",
    "lasso.fit(X_train_pca, y_train)\n",
    "lasso_preds = lasso.predict(X_test_pca)\n",
    "\n",
    "# Evaluate \n",
    "lasso_rmse = root_mean_squared_error(y_test, lasso_preds)\n",
    "lasso_r2 = r2_score(y_test, lasso_preds)\n",
    "\n",
    "print(\"Lasso RMSE:\", lasso_rmse)\n",
    "print(\"Lasso R2:\", lasso_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d7638",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "knn = ...\n",
    "\n",
    "# Fit and predict  \n",
    "knn.fit(X_train_pca, y_train)\n",
    "y_pred_knn = knn.predict(X_test_pca)\n",
    "\n",
    "# Evaluate\n",
    "knn_rmse = root_mean_squared_error(y_test, y_pred_knn)\n",
    "knn_r2 = r2_score(y_test, y_pred_knn)\n",
    "\n",
    "print(\"RMSE:\", knn_rmse)\n",
    "print(\"R2:\", knn_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99391a00",
   "metadata": {},
   "source": [
    "The `n_neighbors` parameter is the number of data points closest to the new point to consider when making a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039cc249",
   "metadata": {},
   "source": [
    "### Decision Tree Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "dt = ...\n",
    "\n",
    "# Fit and predict \n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "dt_rmse = root_mean_squared_error(y_test, y_pred_dt)\n",
    "dt_r2 = r2_score(y_test, y_pred_dt)\n",
    "\n",
    "print(\"Decision Tree RMSE:\", dt_rmse)\n",
    "print(\"Decision Tree R2:\", dt_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea484f",
   "metadata": {},
   "source": [
    "We haven't set a `max_depth` for the tree, so it will continue splitting until each leaf is one sample (and probably overfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce226b42",
   "metadata": {},
   "source": [
    "That was cool, but we're a bit limited by only being able to choose one set of hyperparameters each time..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0ffa7",
   "metadata": {},
   "source": [
    "## Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68972899",
   "metadata": {},
   "source": [
    "### What is GridSearchCV?\n",
    "\n",
    "GridSearchCV is a tool in scikit-learn for **hyperparameter tuning**, which finds the best combination of hyperparameters (e.g., `n_neighbors` in KNN, `max_depth` in decision trees) to improve model performance.\n",
    "\n",
    "### How It Works:\n",
    "1. **Define a Parameter Grid**: Specify ranges for hyperparameters.\n",
    "2. **Cross-Validation**: Train and evaluate the model on multiple data splits for each combination.\n",
    "3. **Select Best Parameters**: Choose the combination with the best performance.\n",
    "\n",
    "Next, we’ll use GridSearchCV to tune a model and evaluate its performance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f879606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define X and y\n",
    "X = df[['age', 'sex', 'bmi', 'children', 'smoker', 'region']]\n",
    "y = df['charges']\n",
    "\n",
    "# 2) Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067436e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Define preprocessing for numerical and categorical features\n",
    "numeric_features = ['age', 'bmi', 'children']\n",
    "categorical_features = ['sex', 'smoker', 'region']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer_num', ...)\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer_cat', ...),\n",
    "    ('onehot', ...)\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# 4) Pipeline: preprocessing + DecisionTreeRegressor\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', ...),\n",
    "    ('tree', DecisionTreeRegressor(random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'tree__max_depth': [3, 5, 10, None],\n",
    "    'tree__min_samples_split': [2, 5, 10],\n",
    "    'tree__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# 6) Set up GridSearchCV with the pipeline and parameter grid\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "# 7) Fit GridSearchCV on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV RMSE:\", -grid_search.best_score_)\n",
    "\n",
    "# 8) Evaluate best model on test set\n",
    "best_model = ...\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Test RMSE:\", root_mean_squared_error(y_test, y_pred))\n",
    "print(\"Test R2:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff01bfb",
   "metadata": {},
   "source": [
    "### Group Activity: Titanic Survival Prediction with Preprocessing + GridSearchCV\n",
    "\n",
    "#### Objective\n",
    "You will use the Titanic dataset to build a complete machine learning workflow that:\n",
    "1) selects features and the target variable  \n",
    "2) splits the data into train/test sets  \n",
    "3) builds a preprocessing pipeline for numeric and categorical features  \n",
    "4) trains a Decision Tree model inside a Pipeline  \n",
    "5) uses GridSearchCV to tune hyperparameters using cross-validation  \n",
    "6) evaluates the tuned model on the test set\n",
    "\n",
    "This assignment focuses on building a correct **scikit-learn Pipeline** and using **GridSearchCV**.\n",
    "\n",
    "\n",
    "#### Dataset Columns\n",
    "#### Response\n",
    "\n",
    "- `Survived`: ***[int64]*** - Survival (0 = No, 1 = Yes)\n",
    "\n",
    "\n",
    "#### Features\n",
    "\n",
    "- `Pclass`: ***[int64]*** - Passenger Class (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "- `Sex`: ***[object]*** - Sex (male, female)\n",
    "- `Age`: ***[float64]*** - Age (in years)\n",
    "- `SibSp`: ***[int64]*** - Number of Siblings/Spouses Aboard\n",
    "- `Parch`: ***[int64]*** - Number of Parents/Children Aboard\n",
    "- `Fare`: ***[float64]*** - Fare (in British pounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf4ca9",
   "metadata": {},
   "source": [
    "### Important: Classification vs Regression\n",
    "\n",
    "The target variable is:\n",
    "\n",
    "- `Survived` → 0 = No, 1 = Yes\n",
    "\n",
    "This is a **binary classification problem**, NOT a regression problem.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "- You must use a **classifier** (e.g., `DecisionTreeClassifier`)\n",
    "- You must NOT use a regressor (e.g., `DecisionTreeRegressor`)\n",
    "- Your evaluation metric should be **accuracy**, not RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset \n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "\n",
    "# Select features and target variable\n",
    "X = data[[]]\n",
    "y = data[[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b8523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing for numerical and categorical features\n",
    "\n",
    "\n",
    "# Combine preprocessors in a column transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdf7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "\n",
    "\n",
    "# Create a pipeline with the preprocessor and Decision Tree model\n",
    "\n",
    "\n",
    "# Set up GridSearchCV with the pipeline and parameter grid\n",
    "\n",
    "\n",
    "# Fit GridSearchCV on the training data\n",
    "\n",
    "\n",
    "# Evaluate the best model on the test set\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
